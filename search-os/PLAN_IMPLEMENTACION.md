# üèóÔ∏è PLAN DE IMPLEMENTACI√ìN - SEARCH OS

**Versi√≥n:** 1.0
**Fecha:** 2025-01-08
**Estado:** Pendiente de Inicio

---

## üìã √çNDICE

1. [Resumen Ejecutivo](#resumen-ejecutivo)
2. [Arquitectura General](#arquitectura-general)
3. [Fase 0: Setup y Arquitectura Base](#fase-0-setup-y-arquitectura-base)
4. [Fase 1: Tool 2 - Data Viewer (Base)](#fase-1-tool-2---data-viewer-base)
5. [Fase 2: Tool 2 - Enriquecimiento IA](#fase-2-tool-2---enriquecimiento-ia)
6. [Fase 3: Tool 2 - UX Polish](#fase-3-tool-2---ux-polish)
7. [Fase 4: Tool 1 - Discovery Engine (Base)](#fase-4-tool-1---discovery-engine-base)
8. [Fase 5: Tool 1 - Integraci√≥n Manifiesto](#fase-5-tool-1---integraci√≥n-manifiesto)
9. [Fase 6: Integraci√≥n y Launcher](#fase-6-integraci√≥n-y-launcher)
10. [Cronograma y Priorizaci√≥n](#cronograma-y-priorizaci√≥n)

---

## üìä RESUMEN EJECUTIVO

### Objetivo General
Construir un sistema completo de herramientas de inversi√≥n (Search OS) con dos componentes principales:
- **Tool 1: Discovery Engine** - Copiloto de inversi√≥n automatizado para an√°lisis sectorial
- **Tool 2: Data Viewer** - Visualizador inteligente de CSVs con enriquecimiento IA

### Reglas de Oro (Hard Constraints)
1. ‚úÖ **Fuente de Verdad**: Usar PDFs solo para l√≥gica de negocio, ignorar c√≥digo obsoleto
2. ‚úÖ **Arquitectura de Datos**: Archivos `.parquet` locales (NO SQL)
3. ‚úÖ **Rendimiento Cr√≠tico**: AgGrid con virtualizaci√≥n obligatoria (paginaci√≥n 50 filas)
4. ‚úÖ **Manifiesto**: Se implementar√° en Fase 5, cuando Tool 1 est√© 100% funcional

### Stack Tecnol√≥gico
- **Frontend**: Streamlit (multi-page)
- **Backend**: Python 3.10+
- **Datos**: Pandas, PyArrow, Fastparquet
- **Grid**: streamlit-aggrid (con virtualizaci√≥n)
- **IA**: Google Gemini 1.5 Pro, Groq, DeepInfra, OpenAI (GPT-4o), Tavily
- **PDFs**: PyPDF2 (solo para extracci√≥n inicial)

---

## üèõÔ∏è ARQUITECTURA GENERAL

### Estructura de Directorios Final

```
search-os/
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ Tool_1_Discovery.pdf
‚îÇ   ‚îú‚îÄ‚îÄ Tool_2_DataViewer.pdf
‚îÇ   ‚îú‚îÄ‚îÄ ROADMAP_DESARROLLO.pdf
‚îÇ   ‚îî‚îÄ‚îÄ extracted/              # Textos extra√≠dos (ya completado)
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ tool_1_discovery/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.py                    # UI Split-Screen
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classifier.py         # CNAE mapping
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ researcher.py         # Tavily integration
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ analyst.py            # Gemini analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ initial_analysis.py   # Prompt maestro
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chat_system.py        # Prompt copiloto
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rules_engine.py           # Hard constraints (sin Manifiesto por ahora)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ report_generator.py       # JSON structure
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ tool_2_dataviewer/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.py                    # UI Principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ csv_loader.py             # Ingesta SABI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aggrid_config.py          # Config AgGrid
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_factory.py            # Factory pattern
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ smart_context.py          # Optimizador
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ai_columns.py             # Columnas IA
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ shared/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_manager.py           # CRUD Parquet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parquet_manager.py        # Utilidades
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # Config global
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py                  # Helpers
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ main.py                       # Launcher multi-page
‚îÇ
‚îî‚îÄ‚îÄ data/
    ‚îú‚îÄ‚îÄ raw/                          # CSVs originales
    ‚îú‚îÄ‚îÄ processed/                    # Parquets por proyecto
    ‚îÇ   ‚îî‚îÄ‚îÄ {project_id}/
    ‚îÇ       ‚îú‚îÄ‚îÄ master_data.parquet
    ‚îÇ       ‚îî‚îÄ‚îÄ schema_config.json
    ‚îî‚îÄ‚îÄ cache/                        # Cache temporal
```

---

## üîß FASE 0: SETUP Y ARQUITECTURA BASE

**Objetivo**: Establecer la infraestructura de datos y gesti√≥n de proyectos

**Duraci√≥n Estimada**: 1-2 d√≠as

### TICKET-00: Configuraci√≥n Inicial del Proyecto
**Tipo**: Setup
**Prioridad**: Cr√≠tica

**Tareas**:
- [ ] Verificar estructura de directorios
- [ ] Configurar `requirements.txt` con todas las dependencias
- [ ] Crear `env.template` con variables de entorno necesarias
- [ ] Configurar `.gitignore` apropiado
- [ ] Crear `README.md` con instrucciones de setup

**Criterios de Aceptaci√≥n**:
- ‚úÖ Proyecto se puede clonar y configurar en un entorno limpio
- ‚úÖ Todas las dependencias se instalan correctamente

---

### TICKET-01: Data Manager Backend
**Tipo**: Backend / Core
**Prioridad**: Cr√≠tica

**Descripci√≥n**:
Crear el m√≥dulo central de gesti√≥n de datos que maneja la persistencia en archivos Parquet y JSON.

**Requerimientos T√©cnicos**:

**Archivo**: `src/shared/data_manager.py`

```python
# Funciones principales:
- create_project(name: str) -> str
  # Crea carpeta en data/processed/{project_id}/
  # Genera schema_config.json por defecto
  # Retorna project_id

- save_master_data(project_id: str, df: pd.DataFrame) -> None
  # Guarda DataFrame como master_data.parquet
  # Usa compresi√≥n snappy o gzip

- load_master_data(project_id: str) -> pd.DataFrame
  # Lee master_data.parquet
  # Retorna DataFrame con tipos preservados

- update_schema(project_id: str, new_config: dict) -> None
  # Actualiza schema_config.json
  # Valida estructura JSON

- list_projects() -> List[str]
  # Lista todos los project_id disponibles
```

**Archivo**: `src/shared/parquet_manager.py`

```python
# Utilidades para manejo de Parquet:
- normalize_column_names(df: pd.DataFrame) -> pd.DataFrame
- add_system_columns(df: pd.DataFrame) -> pd.DataFrame
  # A√±ade _uid (UUID) y _list_id ('inbox' por defecto)
```

**Estructura `schema_config.json` inicial**:
```json
{
  "lists": [
    { "id": "inbox", "name": "üì• Bandeja de Entrada" },
    { "id": "shortlist", "name": "‚≠ê Shortlist" },
    { "id": "discarded", "name": "üóë Descartados" }
  ],
  "custom_columns_definitions": {}
}
```

**Criterios de Aceptaci√≥n**:
- ‚úÖ Se pueden crear proyectos program√°ticamente
- ‚úÖ Se puede guardar y recuperar DataFrames manteniendo tipos (float/int/str)
- ‚úÖ El schema_config.json se crea con estructura correcta
- ‚úÖ Los UUIDs se generan correctamente

---

## üìä FASE 1: TOOL 2 - DATA VIEWER (BASE)

**Objetivo**: Construir el visualizador de CSVs con AgGrid virtualizado y gesti√≥n b√°sica

**Duraci√≥n Estimada**: 3-4 d√≠as

### SPRINT 1.1: Data Engine & Grid B√°sico

#### TICKET-02: Interfaz de Gesti√≥n de Proyectos (Landing)
**Tipo**: Frontend / UI
**Prioridad**: Alta

**Descripci√≥n**:
Pantalla inicial que permite crear o cargar proyectos existentes.

**Archivo**: `src/tool_2_dataviewer/app.py` (secci√≥n inicial)

**Requerimientos T√©cnicos**:
- Sidebar de Streamlit:
  - Lista carpetas existentes en `data/processed/`
  - Al hacer clic, carga `project_id` en `st.session_state['project_id']`
- √Årea Principal (si no hay proyecto seleccionado):
  - Input Text: "Nombre del Nuevo Proyecto"
  - Bot√≥n: "Crear"
  - Al crear, llama a `data_manager.create_project()` y redirige

**Criterios de Aceptaci√≥n**:
- ‚úÖ Usuario ve lista de proyectos guardados
- ‚úÖ Usuario puede crear "Sector X" y se genera la carpeta
- ‚úÖ Al seleccionar proyecto, la interfaz cambia para mostrar t√≠tulo del proyecto activo

---

#### TICKET-03: Motor de Ingesta SABI (ETL)
**Tipo**: Backend / Data Processing
**Prioridad**: Alta

**Descripci√≥n**:
Procesar el Excel "sucio" de SABI y normalizarlo al formato interno.

**Archivo**: `src/tool_2_dataviewer/csv_loader.py`

**Requerimientos T√©cnicos**:

```python
def normalize_sabi_data(uploaded_file) -> pd.DataFrame:
    """
    Normaliza datos de SABI al formato interno.

    Mapeo de columnas:
    - "Nombre" ‚Üí name
    - "Direcci√≥n web" ‚Üí website
    - "C√≥digo NIF" ‚Üí nif
    - "Ingresos de explotaci√≥n..." ‚Üí revenue
    - "EBITDA..." ‚Üí ebitda
    - "Resultado del Ejercicio..." ‚Üí net_income
    - "Localidad" ‚Üí city
    - "Descripci√≥n actividad" ‚Üí description

    Columnas de Sistema:
    - _uid: UUID √∫nico por fila
    - _list_id: Valor por defecto 'inbox'
    """
```

**Integraci√≥n en UI**:
- A√±adir `st.file_uploader` en la pantalla principal si el proyecto est√° vac√≠o
- Al subir archivo, ejecutar `normalize_sabi_data()`
- Guardar autom√°ticamente como `master_data.parquet`

**Criterios de Aceptaci√≥n**:
- ‚úÖ Sistema acepta archivo .xlsx de SABI real
- ‚úÖ DataFrame resultante tiene columnas renombradas correctamente
- ‚úÖ Cada fila tiene `_uid` √∫nico
- ‚úÖ Archivo se guarda autom√°ticamente como `master_data.parquet` tras la subida

---

#### TICKET-04: Implementaci√≥n de AgGrid B√°sico y Sidebar
**Tipo**: Frontend / Grid
**Prioridad**: Cr√≠tica

**Descripci√≥n**:
Renderizar la tabla principal usando `streamlit-aggrid` con virtualizaci√≥n obligatoria.

**Archivo**: `src/tool_2_dataviewer/aggrid_config.py`

**Requerimientos T√©cnicos**:

```python
from st_aggrid import AgGrid, GridOptionsBuilder, GridUpdateMode

def render_grid(df_view: pd.DataFrame, custom_defs: dict) -> dict:
    """
    Configura AgGrid con virtualizaci√≥n y funcionalidades avanzadas.
    """
    gb = GridOptionsBuilder.from_dataframe(df_view)

    # 1. HABILITAR SELECCI√ìN Y CHECKBOXES
    gb.configure_selection(selection_mode="multiple", use_checkbox=True)

    # 2. HABILITAR PANEL LATERAL (SideBar) para mover/ocultar columnas
    gb.configure_side_bar()

    # 3. PAGINACI√ìN (VIRTUALIZACI√ìN CR√çTICA)
    gb.configure_pagination(
        pagination=True,
        paginationPageSize=50,  # CR√çTICO: No cargar todas las filas
        paginationAutoPageSize=False
    )

    # 4. CONFIGURAR COLUMNAS CUSTOM (Etiquetas)
    for col_name, config in custom_defs.items():
        if config['type'] == 'single_select':
            gb.configure_column(
                col_name,
                editable=True,
                cellEditor='agSelectCellEditor',
                cellEditorParams={'values': config['options']}
            )

    # 5. OCULTAR COLUMNAS DE SISTEMA
    gb.configure_column("_uid", hide=True)
    gb.configure_column("_list_id", hide=True)

    # 6. FORMATO NUM√âRICO COMPACTO
    # Configurar formatters para revenue, ebitda, etc.
    # Usar valueFormatter JS para mostrar "1.5M‚Ç¨"

    grid_options = gb.build()

    response = AgGrid(
        df_view,
        gridOptions=grid_options,
        enable_enterprise_modules=True,  # Necesario para SideBar completo
        update_mode=GridUpdateMode.MODEL_CHANGED,
        allow_unsafe_jscode=True  # Para formatters personalizados
    )

    return response
```

**Criterios de Aceptaci√≥n**:
- ‚úÖ La tabla carga datos del Parquet
- ‚úÖ Se pueden seleccionar filas con checkboxes
- ‚úÖ Existe panel lateral donde usuario puede arrastrar columnas para reordenar u ocultar
- ‚úÖ Los n√∫meros se ven formateados (no 1500000 sino 1.5M)
- ‚úÖ **CR√çTICO**: Tabla maneja 10,000+ filas sin congelar (paginaci√≥n activa)

---

### SPRINT 1.2: Interacciones Core

#### TICKET-05: Navegaci√≥n por Pesta√±as (Filtrado de Vistas)
**Tipo**: Frontend / Logic
**Prioridad**: Media

**Descripci√≥n**:
Implementar la l√≥gica para ver diferentes "listas" (Inbox, Shortlist, Descartados) usando pesta√±as.

**Requerimientos T√©cnicos**:
- Leer las listas definidas en `schema_config.json`
- Crear Tabs en Streamlit (`st.tabs`) iterando sobre esas listas
- L√≥gica de Filtrado:
  ```python
  # Cuando usuario est√° en Tab "Shortlist"
  df_view = df_master[df_master['_list_id'] == 'shortlist']
  # Pasar df_view al AgGrid
  ```

**Criterios de Aceptaci√≥n**:
- ‚úÖ Al cambiar de pesta√±a, la tabla muestra solo las empresas correspondientes a esa lista
- ‚úÖ Las empresas reci√©n subidas aparecen solo en "Inbox"

---

#### TICKET-06: Acciones en Lote (Mover Filas)
**Tipo**: Fullstack
**Prioridad**: Alta

**Descripci√≥n**:
Permitir al usuario seleccionar filas y moverlas a otra lista.

**Requerimientos T√©cnicos**:

**Frontend**:
- Detectar selecci√≥n de filas en AgGrid (`grid_response['selected_rows']`)
- UI Flotante: Si hay selecci√≥n, mostrar `st.container` debajo de la tabla con:
  - Selectbox: "Mover a..." (Lista de tablas disponibles)
  - Bot√≥n: "Confirmar"

**Backend Logic**:
```python
def move_rows(project_id: str, selected_uids: List[str], target_list_id: str) -> None:
    """
    Mueve filas entre listas actualizando solo _list_id.
    """
    # 1. Cargar master_data.parquet
    df = load_master_data(project_id)

    # 2. Actualizar solo la columna de control _list_id
    df.loc[df['_uid'].isin(selected_uids), '_list_id'] = target_list_id

    # 3. Guardar y recargar
    save_master_data(project_id, df)
    st.session_state['df_master'] = df
    st.rerun()
```

**Criterios de Aceptaci√≥n**:
- ‚úÖ Seleccionar 3 empresas en "Inbox", moverlas a "Shortlist"
- ‚úÖ Al ir a pesta√±a "Shortlist", las empresas est√°n ah√≠
- ‚úÖ Al volver a "Inbox", ya no est√°n

---

#### TICKET-07: Gesti√≥n de Columnas Personalizadas (Manuales)
**Tipo**: Fullstack
**Prioridad**: Media

**Descripci√≥n**:
Permitir al usuario a√±adir nuevas columnas (Tags o Texto) que se persisten en el esquema y en el dataframe.

**Requerimientos T√©cnicos**:

**UI Modal**:
- Bot√≥n "[+ Columna]" que abre un formulario (`st.form`)
- Inputs:
  - Nombre columna
  - Tipo (Radio: "Texto Libre" vs "Etiqueta")
  - Si es Etiqueta: Input text para opciones ("Alta,Media,Baja")

**Backend**:
```python
def add_custom_column(project_id: str, col_name: str, col_type: str, options: List[str] = None) -> None:
    """
    A√±ade columna nueva al master_data.parquet y actualiza schema_config.json.
    """
    # 1. Cargar datos
    df = load_master_data(project_id)
    schema = load_schema(project_id)

    # 2. A√±adir columna al DataFrame (inicializada a None o "")
    df[col_name] = None

    # 3. Actualizar schema_config.json
    if col_type == 'single_select':
        schema['custom_columns_definitions'][col_name] = {
            'type': 'single_select',
            'options': options
        }

    # 4. Guardar ambos
    save_master_data(project_id, df)
    update_schema(project_id, schema)
```

**AgGrid Update**:
- Si columna es tipo "Etiqueta", configurar `cellEditor='agSelectCellEditor'` con opciones guardadas
- Hacer columna `editable=True`

**Criterios de Aceptaci√≥n**:
- ‚úÖ Usuario crea columna "Estado" con opciones "Pendiente, Contactado"
- ‚úÖ La columna aparece en la tabla
- ‚úÖ Usuario puede hacer doble clic en celda de "Estado" y elegir "Contactado" de dropdown
- ‚úÖ Al recargar p√°gina, el valor "Contactado" persiste

---

#### TICKET-08: Consolidaci√≥n de Datos SABI Jer√°rquicos
**Tipo**: Backend / Data Processing
**Prioridad**: Alta

**Descripci√≥n**:
Consolidar datos jer√°rquicos de SABI (m√∫ltiples filas por empresa) en una sola fila por empresa, agrupando datos repetidos en arrays JSON. Esto permite manejar empresas con 36+ filas de informaci√≥n relacionada (accionistas, participadas, etc.) sin perder datos.

**Requerimientos T√©cnicos**:

**Detecci√≥n de Filas Principales vs Secundarias**:
- Filas principales: Tienen datos en columnas clave (nombre, NIF, revenue, etc.)
- Filas secundarias: Tienen valores vac√≠os/NaN en columnas clave pero datos en columnas relacionadas (accionistas, participadas, etc.)

**Algoritmo de Consolidaci√≥n**:
```python
def consolidate_sabi_hierarchical_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    Consolida m√∫ltiples filas de la misma empresa en una sola fila.

    Estrategia:
    1. Identificar filas principales (tienen nombre/NIF/revenue)
    2. Agrupar filas secundarias por empresa (mismo NIF o nombre similar)
    3. Para cada grupo:
       - Mantener datos de fila principal
       - Consolidar columnas con m√∫ltiples valores en arrays JSON
       - Conservar orden original de elementos
    4. Retornar DataFrame con una fila por empresa
    """
    # 1. Identificar columnas clave (empresa principal)
    key_columns = ['name', 'nif', 'revenue', 'ebitda']

    # 2. Identificar filas principales (tienen al menos 2 columnas clave con datos)
    df['_is_main_row'] = df[key_columns].notna().sum(axis=1) >= 2

    # 3. Agrupar por NIF o nombre (normalizado)
    # 4. Para cada grupo, consolidar columnas con m√∫ltiples valores
    # 5. Convertir arrays a JSON strings para almacenamiento en Parquet
```

**Reglas de Consolidaci√≥n**:
- **Mantener nombres originales de columnas** (no renombrar)
- **Conservar orden** de elementos en arrays
- **Sin l√≠mite de elementos** (puede haber 36+ elementos por array)
- **Tipos de datos**: Los arrays JSON pueden contener n√∫meros, texto, fechas, porcentajes (todo como string en JSON)

**Columnas a Consolidar** (ejemplos):
- `accionista_-_nombre` ‚Üí `accionistas: ["Juan", "Pedro", "Mar√≠a"]`
- `accionista_-_%_directo` ‚Üí `accionistas_pct_directo: ["50", "30", "20"]`
- `participada_-_nombre` ‚Üí `participadas: ["Empresa A", "Empresa B"]`

**Formato de Salida**:
- Columnas principales: Mantienen valores originales
- Columnas consolidadas: JSON arrays como strings (compatibles con Parquet)
- Ejemplo: `accionistas: '["Juan", "Pedro", "Mar√≠a"]'`

**Criterios de Aceptaci√≥n**:
- ‚úÖ Empresa con 36 filas se consolida en 1 fila
- ‚úÖ Todos los datos se conservan (sin p√©rdida de informaci√≥n)
- ‚úÖ Arrays JSON mantienen orden original
- ‚úÖ Nombres de columnas originales se preservan
- ‚úÖ Funciona con empresas que tienen 1-100+ filas relacionadas

---

#### TICKET-09: AgGrid sin Paginaci√≥n (Scroll Infinito)
**Tipo**: Frontend / Grid Configuration
**Prioridad**: Alta

**Descripci√≥n**:
Eliminar paginaci√≥n de AgGrid y mostrar todas las filas con scroll infinito, manteniendo virtualizaci√≥n para rendimiento. El usuario quiere ver todas las filas como en Excel, sin p√°ginas.

**Requerimientos T√©cnicos**:

**Modificar `aggrid_config.py`**:
- Eliminar `configure_pagination()` completamente
- Habilitar virtualizaci√≥n autom√°tica de AgGrid (por defecto)
- Configurar altura de grid para permitir scroll vertical
- Mantener todas las dem√°s configuraciones (filtros, ordenamiento, selecci√≥n)

**Cambios Espec√≠ficos**:
```python
# ELIMINAR estas l√≠neas:
gb.configure_pagination(
    paginationPageSize=50,
    paginationAutoPageSize=False
)

# AGREGAR configuraci√≥n de altura para scroll:
grid_options = {
    'domLayout': 'normal',  # Permite scroll vertical
    'rowHeight': 35,
    'headerHeight': 40,
    'suppressPaginationPanel': True,  # Ocultar controles de paginaci√≥n si aparecen
}
```

**Criterios de Aceptaci√≥n**:
- ‚úÖ No hay controles de paginaci√≥n visibles
- ‚úÖ Todas las filas se muestran (scroll infinito)
- ‚úÖ Rendimiento fluido con 1000+ filas (virtualizaci√≥n activa)
- ‚úÖ Sensaci√≥n de "Excel desktop" (scroll continuo hacia abajo)
- ‚úÖ Filtros y ordenamiento funcionan sobre todas las filas

---

## ü§ñ FASE 2: TOOL 2 - ENRIQUECIMIENTO IA

**Objetivo**: Implementar sistema de columnas IA configurables con m√∫ltiples modelos

**Duraci√≥n Estimada**: 3-4 d√≠as

### SPRINT 2.1: LLM Factory & Smart Context

#### TICKET-10: LLM Factory Pattern
**Tipo**: Backend / AI Integration
**Prioridad**: Alta

**Descripci√≥n**:
Implementar patr√≥n Factory para instanciar el cliente LLM correcto seg√∫n la elecci√≥n del usuario.

**Archivo**: `src/tool_2_dataviewer/llm_factory.py`

**Requerimientos T√©cnicos**:

```python
from langchain_groq import ChatGroq
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI

class LLMFactory:
    @staticmethod
    def get_model(ui_selection: str, api_keys: dict):
        """
        ui_selection puede ser:
        - "instant": Groq (Llama-3-70B)
        - "batch": DeepInfra (Meta-Llama-3-70B-Instruct)
        - "complex": OpenAI (GPT-4o)
        - "long_context": Google (Gemini-1.5-Pro)
        """
        if ui_selection == "instant":
            return ChatGroq(
                model_name="llama-3-70b-8192",
                groq_api_key=api_keys['GROQ'],
                temperature=0.1
            )
        elif ui_selection == "batch":
            return ChatOpenAI(
                model="meta-llama/Meta-Llama-3-70B-Instruct",
                api_key=api_keys['DEEPINFRA'],
                base_url="https://api.deepinfra.com/v1/openai",
                temperature=0.1
            )
        elif ui_selection == "complex":
            return ChatOpenAI(
                model="gpt-4o",
                api_key=api_keys['OPENAI'],
                temperature=0.2
            )
        elif ui_selection == "long_context":
            return ChatGoogleGenerativeAI(
                model="gemini-1.5-pro",
                google_api_key=api_keys['GOOGLE'],
                temperature=0.2
            )
```

**Criterios de Aceptaci√≥n**:
- ‚úÖ Se pueden instanciar los 4 tipos de modelos correctamente
- ‚úÖ Las API keys se cargan desde variables de entorno
- ‚úÖ Cada modelo tiene configuraci√≥n de temperatura apropiada

---

#### TICKET-11: Smart Context Optimizer
**Tipo**: Backend / AI Optimization
**Prioridad**: Media

**Descripci√≥n**:
Agente intermedio que reduce consumo de tokens seleccionando solo las columnas necesarias antes de enviar al LLM principal.

**Archivo**: `src/tool_2_dataviewer/smart_context.py`

**Requerimientos T√©cnicos**:

```python
CONTEXT_OPTIMIZER_SYSTEM_PROMPT = """
ERES UN INGENIERO DE DATOS OPTIMIZADOR.
TU OBJETIVO: Seleccionar del dataset √∫nicamente las columnas estrictamente necesarias para responder al prompt del usuario.

INPUTS: Prompt Usuario + Lista de Headers del CSV
REGLA: S√© minimalista. Incluye siempre 'Nombre' y 'Web'. Devuelve JSON Array de strings.
"""

def optimize_context(user_prompt: str, available_columns: List[str]) -> List[str]:
    """
    Usa Llama-3-8B (Groq) para determinar qu√© columnas son necesarias.
    """
    # Llamada a Groq con prompt optimizador
    # Retorna lista de columnas seleccionadas
```

**Criterios de Aceptaci√≥n**:
- ‚úÖ Para prompt "Analiza edad administradores", solo selecciona columnas relevantes
- ‚úÖ Siempre incluye 'Nombre' y 'Web'
- ‚úÖ Reduce tokens en 60-80% vs enviar todas las columnas

---

#### TICKET-12: Modal Creaci√≥n Columna IA
**Tipo**: Frontend / UI
**Prioridad**: Alta

**Descripci√≥n**:
Interfaz para crear columnas inteligentes configurando prompt y modelo.

**Requerimientos T√©cnicos**:

**UI Modal** (`st.form`):
- Input: T√≠tulo de la Columna (ej. "Riesgo Regulatorio")
- Text Area: Prompt (Instrucciones para la IA)
- Selector de Modelo (Dropdown):
  - ‚ö° Instant√°neo: Groq (Llama-3-70B) - Default
  - üè≠ Batch/Econ√≥mico: DeepInfra (Llama-3/Mistral)
  - üß† Razonamiento Complejo: OpenAI (GPT-4o)
  - üìö Contexto Largo: Google (Gemini-1.5-Pro)
- Switch: "Auto-optimizar columnas (Smart Context)" (Activado por defecto)

**Backend**:
- Guardar definici√≥n en `schema_config.json`:
```json
{
  "field": "ai_relevo",
  "title": "Relevo Generacional",
  "type": "ai_score",
  "config": {
    "user_prompt": "Analiza edad administradores...",
    "model_selected": "instant",
    "smart_context": true
  }
}
```

**Criterios de Aceptaci√≥n**:
- ‚úÖ Usuario puede crear columna IA con t√≠tulo y prompt
- ‚úÖ Puede seleccionar modelo seg√∫n necesidad
- ‚úÖ La configuraci√≥n se guarda en schema_config.json

---

### SPRINT 2.2: Pipeline de Ejecuci√≥n

#### TICKET-13: Pipeline Batch de Enriquecimiento
**Tipo**: Backend / AI Processing
**Prioridad**: Alta

**Descripci√≥n**:
Procesar enriquecimiento IA por lotes sobre m√∫ltiples filas.

**Archivo**: `src/tool_2_dataviewer/ai_columns.py`

**Requerimientos T√©cnicos**:

```python
def enrich_column_batch(
    project_id: str,
    column_def: dict,
    row_indices: List[int] = None  # None = todas las filas
) -> None:
    """
    Ejecuta enriquecimiento IA sobre filas seleccionadas.
    """
    # 1. Cargar datos
    df = load_master_data(project_id)

    # 2. Smart Context: Determinar columnas necesarias
    if column_def['config']['smart_context']:
        needed_cols = optimize_context(
            column_def['config']['user_prompt'],
            df.columns.tolist()
        )
    else:
        needed_cols = df.columns.tolist()

    # 3. Construir JSON ligero por fila
    rows_to_process = df.iloc[row_indices] if row_indices else df

    for idx, row in rows_to_process.iterrows():
        # Construir contexto JSON m√≠nimo
        context = {col: row[col] for col in needed_cols}

        # 4. (Opcional) B√∫squeda Tavily si es necesario
        # web_data = tavily_search(row['website']) if 'website' in context

        # 5. Llamada al LLM
        model = LLMFactory.get_model(
            column_def['config']['model_selected'],
            api_keys
        )

        prompt = f"""
        {column_def['config']['user_prompt']}

        DATOS EMPRESA:
        {json.dumps(context, indent=2)}

        Devuelve un JSON con:
        - score: n√∫mero 0-10
        - reason: explicaci√≥n breve
        """

        response = model.invoke(prompt)
        result = parse_json_response(response.content)

        # 6. Actualizar DataFrame
        df.loc[idx, column_def['field']] = result['score']
        df.loc[idx, f"{column_def['field']}_reason"] = result['reason']

    # 7. Guardar
    save_master_data(project_id, df)
```

**Criterios de Aceptaci√≥n**:
- ‚úÖ Procesa 1000+ filas en lotes sin congelar la app
- ‚úÖ Muestra progreso con `st.progress`
- ‚úÖ Los scores y razones se guardan en el Parquet
- ‚úÖ Si falla una fila, contin√∫a con las dem√°s

---

#### TICKET-14: Renderizado IA Scores
**Tipo**: Frontend / Grid
**Prioridad**: Media

**Descripci√≥n**:
Mostrar scores IA con badges visuales y tooltips.

**Requerimientos T√©cnicos**:

**Cell Renderer Personalizado** (JavaScript en AgGrid):
```javascript
function ScoreBadgeRenderer(params) {
    const score = params.value;
    let color, emoji;

    if (score >= 8) {
        color = 'green';
        emoji = 'üü¢';
    } else if (score >= 5) {
        color = 'yellow';
        emoji = 'üü°';
    } else {
        color = 'red';
        emoji = 'üî¥';
    }

    return `
        <div style="display: flex; align-items: center; gap: 5px;">
            <span>${emoji}</span>
            <span style="font-weight: bold;">${score}/10</span>
        </div>
    `;
}
```

**Tooltip**:
- Usar `tooltipField` en AgGrid apuntando a `{field}_reason`

**Criterios de Aceptaci√≥n**:
- ‚úÖ Scores se muestran con badges de color (üü¢üü°üî¥)
- ‚úÖ Al pasar mouse, muestra tooltip con justificaci√≥n
- ‚úÖ Formato es claro y visual

---

## üé® FASE 3: TOOL 2 - UX POLISH

**Objetivo**: Mejoras de UX y funcionalidades avanzadas

**Duraci√≥n Estimada**: 2-3 d√≠as

#### TICKET-15: Data Chat (Asistente Lateral)
**Tipo**: Frontend / AI Integration
**Prioridad**: Media

**Descripci√≥n**:
Panel desplegable a la derecha que permite "hablar" con los datos visibles.

**Requerimientos T√©cnicos**:
- Panel lateral (`st.sidebar` o `st.expander`)
- Usa Groq por defecto para respuestas instant√°neas
- Contexto: Solo filas visibles en la tabla actual
- Ejemplo: "F√≠ltrame las empresas de Valencia con EBITDA > 1M‚Ç¨"

**Criterios de Aceptaci√≥n**:
- ‚úÖ Usuario puede hacer preguntas sobre datos visibles
- ‚úÖ Respuestas son instant√°neas (Groq)
- ‚úÖ Puede generar filtros y acciones

---

#### TICKET-16: Action Bar Flotante Mejorado
**Tipo**: Frontend / UI
**Prioridad**: Media

**Descripci√≥n**:
Mejorar la barra de acciones que aparece al seleccionar filas.

**Requerimientos T√©cnicos**:
- Aparece al seleccionar filas (checkboxes)
- Acciones disponibles:
  - Mover a... (dropdown)
  - Borrar (eliminar de master_data)
  - Ejecutar IA (aplicar columna IA a filas seleccionadas)

**Criterios de Aceptaci√≥n**:
- ‚úÖ Barra aparece solo cuando hay selecci√≥n
- ‚úÖ Todas las acciones funcionan correctamente

---

#### TICKET-17: Exportaci√≥n y Descarga
**Tipo**: Backend / Data Export
**Prioridad**: Baja

**Descripci√≥n**:
Permitir exportar datos a CSV/Excel manteniendo formato.

**Criterios de Aceptaci√≥n**:
- ‚úÖ Exportar a CSV mantiene columnas IA
- ‚úÖ Exportar a Excel con formato preservado

---

## üîç FASE 4: TOOL 1 - DISCOVERY ENGINE (BASE)

**Objetivo**: Construir el Copiloto de Inversi√≥n con UI Split-Screen

**Duraci√≥n Estimada**: 4-5 d√≠as

### SPRINT 4.1: Backend Core

#### TICKET-18: Pipeline de Agentes
**Tipo**: Backend / AI Agents
**Prioridad**: Cr√≠tica

**Descripci√≥n**:
Implementar el pipeline de 3 agentes: Clasificador, Investigador, Analista.

**Archivos**:
- `src/tool_1_discovery/agents/classifier.py`
- `src/tool_1_discovery/agents/researcher.py`
- `src/tool_1_discovery/agents/analyst.py`

**Requerimientos T√©cnicos**:

**Clasificador** (CNAE Mapping):
```python
def classify_sector(sector_name: str) -> List[str]:
    """
    Mapea sector a c√≥digos CNAE 2009 (Espa√±a).
    Usa Gemini para mapeo inteligente.
    """
    # Retorna lista de c√≥digos CNAE
```

**Investigador** (Tavily):
```python
def research_sector(sector_name: str, cnae_codes: List[str]) -> str:
    """
    Genera queries de b√∫squeda y ejecuta b√∫squedas Tavily.

    Queries base:
    - "Tama√±o mercado {sector} Espa√±a 2024 facturaci√≥n"
    - "M√°rgenes EBITDA sector {sector} Espa√±a"
    - "Cuota de mercado l√≠deres {sector} Espa√±a"
    - "Normativa y regulaci√≥n {sector} Espa√±a riesgos"
    - "Tendencias M&A {sector} Europa 2024"
    """
    # Retorna contexto web consolidado
```

**Analista** (Gemini):
```python
def generate_initial_report(sector_name: str, web_context: str) -> dict:
    """
    Genera el informe inicial completo en formato JSON.
    Usa INITIAL_ANALYSIS_SYSTEM_PROMPT.
    """
    # Retorna JSON estructurado con 10 secciones
```

**Criterios de Aceptaci√≥n**:
- ‚úÖ Pipeline completo funciona end-to-end
- ‚úÖ JSON generado respeta estructura de 10 secciones
- ‚úÖ B√∫squedas Tavily retornan datos relevantes

---

#### TICKET-17: Sistema de Prompts Maestros
**Tipo**: Backend / AI Prompts
**Prioridad**: Alta

**Descripci√≥n**:
Definir y estructurar los prompts del sistema.

**Archivos**:
- `src/tool_1_discovery/prompts/initial_analysis.py`
- `src/tool_1_discovery/prompts/chat_system.py`

**Requerimientos T√©cnicos**:

**INITIAL_ANALYSIS_SYSTEM_PROMPT**:
- Define rol: Director de Inversiones de Emerita
- Inyecta Tesis Emerita (hardcoded por ahora, sin Manifiesto):
  - TARGET_GEO: "Espa√±a (prioritario), Europa Occidental (secundario)"
  - TARGET_SIZE: "Ventas 5-40M‚Ç¨, EBITDA 1-5M‚Ç¨"
  - TARGET_MARGINS: "EBITDA ‚â•15%, Bruto ‚â•40%"
  - DEAL_KILLERS: "Riesgo tecnol√≥gico alto, Dependencia de un solo cliente, Sector en declive, M√°rgenes muy bajos (<10%)"
  - VALUE_LEVERS: "Digitalizaci√≥n, Profesionalizaci√≥n comercial, Eficiencia operativa"
- Estructura JSON obligatoria (10 secciones)
- Tono: Profesional, esc√©ptico, basado en datos

**CHAT_SYSTEM_PROMPT**:
- Define rol: Copiloto de Inversi√≥n
- Herramientas disponibles (function calling):
  - `read_full_document()`: Lee contenido actual
  - `search_internet(query)`: Usa Tavily para b√∫squedas frescas
  - `update_section(section_key, user_instruction)`: Reescribe UNA secci√≥n espec√≠fica
- Reglas de comportamiento

**Criterios de Aceptaci√≥n**:
- ‚úÖ Prompts est√°n bien estructurados y documentados
- ‚úÖ Tesis Emerita est√° inyectada correctamente
- ‚úÖ Function calling est√° definido

---

#### TICKET-18: Motor de Reglas (Rules Engine)
**Tipo**: Backend / Business Logic
**Prioridad**: Alta

**Descripci√≥n**:
Implementar motor de reglas para evaluar sectores seg√∫n criterios Emerita.

**Archivo**: `src/tool_1_discovery/rules_engine.py`

**Requerimientos T√©cnicos**:

```python
def evaluate_sector(margins_data: dict, market_data: dict) -> dict:
    """
    Eval√∫a sector y retorna veredicto (VERDE/√ÅMBAR/ROJO).

    Reglas:
    - EBITDA < 15% ‚Üí ROJO o √ÅMBAR
    - Margen Bruto < 40% ‚Üí Penalizaci√≥n
    - Top 3 > 50% cuota ‚Üí Penalizaci√≥n (poco fragmentado)
    - Deal Killers detectados ‚Üí ROJO
    """
    # Retorna {'verdict': 'VERDE/√ÅMBAR/ROJO', 'reasons': [...]}
```

**Criterios de Aceptaci√≥n**:
- ‚úÖ Detecta correctamente Deal Killers
- ‚úÖ Asigna veredicto correcto seg√∫n m√©tricas
- ‚úÖ Genera razones justificadas

---

### SPRINT 4.2: UI Split-Screen

#### TICKET-19: Layout Pantalla Dividida
**Tipo**: Frontend / UI
**Prioridad**: Alta

**Descripci√≥n**:
Implementar UI split-screen con dos paneles.

**Archivo**: `src/tool_1_discovery/app.py`

**Requerimientos T√©cnicos**:
- Layout: `st.columns([1, 2])` para 30-40% / 60-70%
- Panel Izquierdo (30-40%):
  - Estado A: Input inicial (sector name + contexto adicional)
  - Bot√≥n CTA: "ARRANCAR AN√ÅLISIS"
  - Estado B: Chat interactivo (se activa tras primer borrador)
- Panel Derecho (60-70%):
  - Visor de documento Markdown
  - Renderiza JSON estructurado secci√≥n por secci√≥n

**Criterios de Aceptaci√≥n**:
- ‚úÖ Layout se divide correctamente
- ‚úÖ Panel izquierdo cambia de estado A a B
- ‚úÖ Panel derecho renderiza Markdown correctamente

---

#### TICKET-20: Session State JSON
**Tipo**: Backend / State Management
**Prioridad**: Alta

**Descripci√≥n**:
Gestionar estado de la aplicaci√≥n en formato JSON modular.

**Requerimientos T√©cnicos**:

```python
# Estructura en st.session_state['discovery_report']
{
    "meta": {
        "sector_name": "Log√≠stica de Fr√≠o",
        "cnae_codes": ["5210"],
        "verdict": "√ÅMBAR",
        "timestamp": "2025-01-08"
    },
    "sections": {
        "1_executive_summary": {"title": "...", "content": "..."},
        "2_financials": {"title": "...", "content": "..."},
        # ... 10 secciones totales
    }
}
```

**Criterios de Aceptaci√≥n**:
- ‚úÖ JSON se inicializa correctamente
- ‚úÖ Secciones se pueden actualizar independientemente
- ‚úÖ Estado persiste durante la sesi√≥n

---

#### TICKET-21: Chat Interactivo
**Tipo**: Frontend / AI Integration
**Prioridad**: Alta

**Descripci√≥n**:
Implementar chat con function calling para manipular documento.

**Requerimientos T√©cnicos**:
- Chat usa Gemini con function calling:
  - `read_full_document()`: Lee JSON actual
  - `search_internet(query)`: B√∫squeda Tavily
  - `update_section(section_key, user_instruction)`: Reescribe secci√≥n
- Al actualizar secci√≥n, solo esa secci√≥n muestra spinner
- Resto del documento permanece est√°tico

**Criterios de Aceptaci√≥n**:
- ‚úÖ Usuario puede hacer preguntas sobre el documento
- ‚úÖ Usuario puede pedir cambios ("A√±ade esto a secci√≥n 3")
- ‚úÖ Solo la secci√≥n afectada se actualiza
- ‚úÖ Refresh autom√°tico del panel derecho

---

### SPRINT 4.3: Generaci√≥n y Exportaci√≥n

#### TICKET-22: Generaci√≥n de Queries Tavily
**Tipo**: Backend / Research
**Prioridad**: Media

**Descripci√≥n**:
Implementar funci√≥n que genera queries de b√∫squeda optimizadas.

**Archivo**: `src/tool_1_discovery/agents/researcher.py`

**Requerimientos T√©cnicos**:

```python
def get_research_queries(sector: str) -> List[str]:
    """
    Genera lista de queries optimizadas para Tavily.
    """
    return [
        f"Tama√±o mercado {sector} Espa√±a 2024 facturaci√≥n",
        f"M√°rgenes EBITDA sector {sector} Espa√±a",
        f"Principales empresas {sector} Espa√±a cuota mercado",
        f"Asociaci√≥n nacional empresas {sector} Espa√±a",
        f"Normativa y regulaci√≥n {sector} Espa√±a riesgos",
        f"Tendencias M&A {sector} Europa 2024"
    ]
```

**Criterios de Aceptaci√≥n**:
- ‚úÖ Queries son relevantes y espec√≠ficas
- ‚úÖ Retornan resultados √∫tiles de Tavily

---

#### TICKET-23: Exportaci√≥n de Informes
**Tipo**: Backend / Export
**Prioridad**: Baja

**Descripci√≥n**:
Permitir descargar informes en formato Markdown y JSON.

**Criterios de Aceptaci√≥n**:
- ‚úÖ Descarga Markdown (.md) formateado para Notion
- ‚úÖ Descarga JSON estructurado
- ‚úÖ Formato es limpio y profesional

---

## üìÑ FASE 5: TOOL 1 - INTEGRACI√ìN MANIFIESTO

**Objetivo**: Integrar Manifiesto.pdf como fuente de verdad din√°mica

**Duraci√≥n Estimada**: 2-3 d√≠as

**NOTA**: Esta fase se ejecutar√° cuando Tool 1 est√© 100% funcional.

#### TICKET-24: Sistema de Gesti√≥n del Manifiesto
**Tipo**: Fullstack
**Prioridad**: Media

**Descripci√≥n**:
Permitir subir, editar y actualizar el Manifiesto.pdf desde la interfaz.

**Requerimientos T√©cnicos**:
- Editor de texto/markdown en Streamlit
- Guardado como PDF actualizado
- Parser de reglas desde Manifiesto

**Criterios de Aceptaci√≥n**:
- ‚úÖ Usuario puede subir Manifiesto.pdf
- ‚úÖ Puede editar contenido desde interfaz
- ‚úÖ Cambios se guardan como PDF actualizado

---

#### TICKET-25: Actualizaci√≥n Din√°mica de Reglas
**Tipo**: Backend / Business Logic
**Prioridad**: Alta

**Descripci√≥n**:
Cargar reglas desde Manifiesto y actualizar Rules Engine din√°micamente.

**Criterios de Aceptaci√≥n**:
- ‚úÖ Reglas se extraen correctamente del Manifiesto
- ‚úÖ Rules Engine se actualiza sin reiniciar app
- ‚úÖ Validaci√≥n de formato de reglas

---

## üîó FASE 6: INTEGRACI√ìN Y LAUNCHER

**Objetivo**: Unificar ambas herramientas en una app multi-page

**Duraci√≥n Estimada**: 1-2 d√≠as

#### TICKET-26: Launcher Principal
**Tipo**: Frontend / Integration
**Prioridad**: Alta

**Descripci√≥n**:
Crear launcher principal que unifica Tool 1 y Tool 2.

**Archivo**: `src/main.py`

**Requerimientos T√©cnicos**:
- Streamlit multi-page application
- Navegaci√≥n entre Tool 1 y Tool 2
- Gesti√≥n de sesiones unificada

**Criterios de Aceptaci√≥n**:
- ‚úÖ Usuario puede navegar entre herramientas
- ‚úÖ Estado de sesi√≥n se mantiene
- ‚úÖ UI es consistente entre herramientas

---

#### TICKET-27: Compartir Utilidades
**Tipo**: Backend / Refactoring
**Prioridad**: Media

**Descripci√≥n**:
Refactorizar c√≥digo com√∫n entre herramientas.

**Criterios de Aceptaci√≥n**:
- ‚úÖ No hay duplicaci√≥n de c√≥digo
- ‚úÖ Utilidades compartidas funcionan correctamente

---

## üìÖ CRONOGRAMA Y PRIORIZACI√ìN

### Orden de Implementaci√≥n Recomendado

1. **Fase 0** (1-2 d√≠as) ‚Üí Base para todo
2. **Fase 1** (3-4 d√≠as) ‚Üí Tool 2 Base (funcionalidad core sin IA)
3. **Fase 2** (3-4 d√≠as) ‚Üí Tool 2 IA (enriquecimiento inteligente)
4. **Fase 3** (2-3 d√≠as) ‚Üí Tool 2 Polish (UX avanzada)
5. **Fase 4** (4-5 d√≠as) ‚Üí Tool 1 Base (Discovery engine)
6. **Fase 5** (2-3 d√≠as) ‚Üí Manifiesto (cuando Tool 1 est√© funcional)
7. **Fase 6** (1-2 d√≠as) ‚Üí Integraci√≥n final

**Total Estimado**: 18-25 d√≠as de desarrollo

### Dependencias Cr√≠ticas

- Fase 0 ‚Üí Todas las dem√°s fases
- Fase 1 ‚Üí Fase 2, Fase 3
- Fase 4 ‚Üí Fase 5
- Fase 1-5 ‚Üí Fase 6

---

## ‚ö†Ô∏è CONSIDERACIONES T√âCNICAS CR√çTICAS

### Tool 2 - Rendimiento
- ‚úÖ AgGrid con `pagination=True` (50 filas por p√°gina) - **OBLIGATORIO**
- ‚úÖ Virtualizaci√≥n autom√°tica de AgGrid
- ‚úÖ No cargar todo el DataFrame en memoria de una vez
- ‚úÖ Usar `chunksize` en pandas para CSVs grandes

### Tool 1 - IA
- ‚úÖ Gemini 1.5 Pro para contexto largo (2M tokens)
- ‚úÖ Tavily para b√∫squedas web optimizadas
- ‚úÖ Function calling para manipulaci√≥n de JSON
- ‚úÖ Temperatura baja (0.2-0.3) para an√°lisis riguroso

### Arquitectura de Datos
- ‚úÖ Parquet con compresi√≥n (snappy o gzip)
- ‚úÖ Schema evolution compatible
- ‚úÖ Backup autom√°tico antes de modificaciones

---

## üìù NOTAS FINALES

- Este plan est√° dise√±ado para implementaci√≥n incremental
- Cada ticket debe tener criterios de aceptaci√≥n claros
- Se recomienda hacer code review despu√©s de cada sprint
- Testing manual es suficiente para MVP (no se requiere test automatizado en esta fase)

---

**√öltima Actualizaci√≥n**: 2025-01-08
**Estado**: ‚úÖ Plan Completo - Listo para Implementaci√≥n
